{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.8: Racetrack (programming) \n",
    "Consider driving a race car around a turn like those shown in Figure 5.5. You want to go as fast as possible, but not so fast as to run off the track. In our simplified racetrack, the car is at one of a discrete set of grid positions, the cells in the diagram. \n",
    "* The velocity is also discrete, a number of grid cells moved horizontally and vertically per time step. \n",
    "* The actions are increments to the velocity components. Each may be changed by +1, −1, or 0 in one step, for a total of nine actions. \n",
    "* Both velocity components are restricted to be nonnegative and less than 5, and they cannot both be zero except at the starting line. \n",
    "* Each episode begins in one of the randomly selected start states with both velocity components zero and ends when the car crosses the finish line. \n",
    "* The rewards are −1 for each step until the car crosses the finish line. \n",
    "* If the car hits the track boundary, it is moved back to a random position on the starting line, both velocity components are reduced to zero, and the episode continues. \n",
    "* Before updating the car’s location at each time step, check to see if the projected path of the car intersects the track boundary. If it intersects the finish line, the episode ends; if it intersects anywhere else, the car is considered to have hit the track boundary and is sent back to the starting line. \n",
    "* To make the task more challenging, with probability 0.1 at each time step the velocity increments are both zero, independently of the intended increments. \n",
    "\n",
    "Apply a Monte Carlo control method to this task to compute the optimal policy from each starting state. Exhibit several trajectories following the optimal policy (but turn the noise off for these trajectories).\n",
    "\n",
    "<img src=\"racetrack.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import os\n",
    "\n",
    "CELL_TYPE_WALL = 0\n",
    "CELL_TYPE_TRACK = 1\n",
    "CELL_TYPE_GOAL = 2\n",
    "CELL_TYPE_START = 3\n",
    "\n",
    "class RaceTrack:\n",
    "    def __init__(self, track, min_vel=0, max_vel=5):\n",
    "        self.track = track\n",
    "        self.wall_cells = np.argwhere(track == CELL_TYPE_WALL).tolist()\n",
    "        self.goal_cells = np.argwhere(track == CELL_TYPE_GOAL).tolist()\n",
    "        self.start_cells = np.argwhere(track == CELL_TYPE_START).tolist()\n",
    "        self.min_vel = min_vel\n",
    "        self.max_vel = max_vel\n",
    "    \n",
    "    @classmethod\n",
    "    def from_csv(cls, file_path):\n",
    "        \n",
    "        file_path = os.path.join(os.getcwd(), file_path)\n",
    "        \n",
    "        track = genfromtxt(file_path, delimiter=',')\n",
    "        \n",
    "        # Flip the y-axis coordinates\n",
    "        track = np.flip(track, axis=0)\n",
    "\n",
    "        return cls(track) \n",
    "    \n",
    "    def possible_actions(self, state):\n",
    "        actions = [[a_y, a_x] for a_y in range(-1, 2) for a_x in range(-1, 2)]\n",
    "        legal_actions = []\n",
    "\n",
    "        _, _, v_y, v_x = state\n",
    "\n",
    "        # Discard illegal actions\n",
    "        for a in actions:\n",
    "            a_y, a_x = a\n",
    "            # Cannot go above speed limit in any x direction\n",
    "            if v_x + a_x < self.min_vel or v_x + a_x > self.max_vel:\n",
    "                continue\n",
    "            # Cannot go above speed limit in any y direction\n",
    "            if v_y + a_y < self.min_vel or v_y + a_y > self.max_vel:\n",
    "                continue\n",
    "            # Cannot noop\n",
    "            if v_x + a_x == 0 and v_y + a_y == 0:\n",
    "                continue\n",
    "            legal_actions.append(a)\n",
    "        return legal_actions\n",
    "    \n",
    "    def random_start_state(self):\n",
    "        start_cell_idx = np.random.choice(len(self.start_cells))\n",
    "        start_state = np.array(self.start_cells[start_cell_idx] + [0, 0])\n",
    "        return start_state\n",
    "    \n",
    "    def apply_action(self, state, action):\n",
    "        y_coord, x_coord, y_vel, x_vel = state\n",
    "        a_y, a_x = action\n",
    "\n",
    "        next_y_vel = y_vel + a_y\n",
    "        next_x_vel = x_vel + a_x\n",
    "        next_y_coord = y_coord + next_y_vel\n",
    "        next_x_coord = x_coord + next_x_vel\n",
    "\n",
    "        path = self.projected_path(\n",
    "            (y_coord, x_coord), (next_y_vel, next_x_vel))\n",
    "\n",
    "        if self.crossed_finish_line(path):\n",
    "            return self.random_start_state(), 0, True\n",
    "        if self.crossed_track_boundary(path):\n",
    "            # if self.crossed_track_boundary([(next_y_coord, next_x_coord)]):\n",
    "            return self.random_start_state(), -1, False\n",
    "\n",
    "        return np.array([next_y_coord, next_x_coord, next_y_vel, next_x_vel]), -1, False\n",
    "        \n",
    "    def projected_path(self, state, speed):\n",
    "        # TODO: Should we only consider end state directly?\n",
    "        y_coord, x_coord = state\n",
    "        y_vel, x_vel = speed\n",
    "\n",
    "        new_y_coord = y_coord + y_vel\n",
    "        new_x_coord = x_coord + x_vel\n",
    "\n",
    "        path = []\n",
    "        for dy in range(min(y_coord, new_y_coord), max(y_coord, new_y_coord) + 1):\n",
    "            for dx in range(min(x_coord, new_x_coord), max(x_coord, new_x_coord) + 1):\n",
    "                path.append([dy, dx])\n",
    "        return path\n",
    "        \n",
    "    def crossed_track_boundary(self, projected_path):\n",
    "        for cell in projected_path:\n",
    "            y, x = cell\n",
    "            if y < 0 or y >= self.track.shape[0] or x < 0 or x >= self.track.shape[1] or cell in self.wall_cells:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def crossed_finish_line(self, projected_path):\n",
    "        for cell in projected_path:\n",
    "            if cell in self.goal_cells:\n",
    "                return True\n",
    "        return False\n",
    "        \n",
    "    def draw(self, car_cell=None, path=[]):        \n",
    "        colors = ['black', 'white', 'yellow', 'red']\n",
    "\n",
    "        im = plt.imshow(self.track, cmap=ListedColormap(colors),\n",
    "                        origin='lower', interpolation='none', animated=True)\n",
    "\n",
    "        def rect(pos, edgecolor='k', facecolor='none'):\n",
    "            r = plt.Rectangle(pos, 1, 1, facecolor=facecolor,\n",
    "                              edgecolor=edgecolor, linewidth=2)\n",
    "            plt.gca().add_patch(r)\n",
    "\n",
    "        for i in range(self.track.shape[0]):\n",
    "            for j in range(self.track.shape[1]):\n",
    "                rect((j-0.5, i-0.5))\n",
    "\n",
    "        if path:\n",
    "            for cell in path:\n",
    "                rect((cell[1]-0.5, cell[0]-0.5), edgecolor='g')\n",
    "\n",
    "        if car_cell:\n",
    "            rect((car_cell[1]-0.5, car_cell[0]-0.5),\n",
    "                 edgecolor='g', facecolor='g')\n",
    "\n",
    "        plt.gca().invert_yaxis()\n",
    "        return im\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt = RaceTrack.from_csv(\"../racetracks/map1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 8], [4, 9], [4, 10], [5, 8], [5, 9], [5, 10], [6, 8], [6, 9], [6, 10]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = rt.projected_path((6,8),(-2,2))\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x111663860>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "rt.draw((6,8),path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "class OnPolicyMonteCarloAgent:\n",
    "    def __init__(self, track, gamma=1, n_episodes=100000, eps=0.5):\n",
    "        self.track = track\n",
    "        self.gamma = gamma\n",
    "        self.n_episodes = n_episodes\n",
    "        self.eps = eps\n",
    "\n",
    "        # Initialize Q values and C values\n",
    "        y_range = track.track.shape[0]\n",
    "        x_range = track.track.shape[1]\n",
    "        yvel_range = track.max_vel - track.min_vel + 1\n",
    "        xvel_range = track.max_vel - track.min_vel + 1\n",
    "        yacc_range = 3  # -1, 0, +1\n",
    "        xacc_range = 3  # -1, 0, +1\n",
    "\n",
    "        # Initialize state-action values\n",
    "        self.Q = np.zeros((y_range, x_range, yvel_range,\n",
    "                           xvel_range, yacc_range, xacc_range))\n",
    "\n",
    "        # Initialize rewards dictionary\n",
    "        self.R = defaultdict(list)\n",
    "\n",
    "        # Initial Policy\n",
    "        # For each state: assign equal probability of selecting each valid action from the state\n",
    "        self.pi = np.zeros(self.Q.shape, dtype=float)\n",
    "        for y_coord in range(self.Q.shape[0]):\n",
    "            for x_coord in range(self.Q.shape[1]):\n",
    "                for y_vel in range(track.min_vel, track.max_vel + 1):\n",
    "                    for x_vel in range(track.min_vel, track.max_vel + 1):\n",
    "                        valid_actions = self.track.possible_actions(\n",
    "                            (y_coord, x_coord, y_vel, x_vel))\n",
    "                        for y_acc, x_acc in valid_actions:\n",
    "                            self.pi[y_coord, x_coord, y_vel, x_vel,\n",
    "                                    y_acc, x_acc] = 1/len(valid_actions)\n",
    "                        \n",
    "    def sample_random_action(self, state):\n",
    "        # Sample action according to our eps-greedy policy\n",
    "        # Ensure that probabilities we sample from sum to 1\n",
    "        y_coord, x_coord, y_vel, x_vel = state\n",
    "\n",
    "        actionprobs = self.pi[y_coord, x_coord, y_vel, x_vel]\n",
    "        total_prob = np.sum(actionprobs)\n",
    "        if not math.isclose(total_prob, 1, abs_tol=0.01):\n",
    "            print(\n",
    "                'Action probabilities must sum to 1.0, but summed to {}, state: {}, actionprobs: {}'.format(total_prob, state, self.pi[tuple(state)]))\n",
    "            sys.exit(1)\n",
    "\n",
    "        linear_idx = np.random.choice(\n",
    "            actionprobs.size, p=actionprobs.ravel())\n",
    "        a = np.unravel_index(linear_idx, actionprobs.shape)\n",
    "        # In case the value is greater than the max allowed action we need to translate it back into\n",
    "        # negative coordinates\n",
    "        a = tuple(acc if acc <= 1 else 1 - acc for acc in a)\n",
    "        return a  \n",
    "            \n",
    "    def generate_episode(self, pi):\n",
    "        S = []\n",
    "        A = []\n",
    "        R = []\n",
    "\n",
    "        # Select the initial state randomly\n",
    "        start_state = self.track.random_start_state()\n",
    "        S.append(start_state)\n",
    "\n",
    "        terminated = False\n",
    "        t = 0\n",
    "        while not terminated:\n",
    "            St = S[t]\n",
    "\n",
    "            if t % 10000 == 0:\n",
    "                print(\"Step: {}\".format(t))\n",
    "\n",
    "            a = self.sample_random_action(St)\n",
    "            A.append(a)\n",
    "\n",
    "            next_state, reward, terminated = self.track.apply_action(St, a)\n",
    "\n",
    "            R.append(reward)\n",
    "            S.append(next_state)\n",
    "            t += 1\n",
    "\n",
    "            # time.sleep(1)\n",
    "        print(\"Terminated after {} steps\".format(t))\n",
    "        return S, A, R\n",
    "    \n",
    "    def policy_iteration(self):\n",
    "        policy_stable = False\n",
    "        it = 0\n",
    "        while not policy_stable or it < self.n_episodes:\n",
    "            print('Iteration {}'.format(it))\n",
    "\n",
    "            # (a) Generate an episode using pi\n",
    "            S, A, R = self.generate_episode(self.pi)\n",
    "\n",
    "            visited = set()\n",
    "\n",
    "            # (b) Iterate over s,a pairs and update rewards and q-values\n",
    "            old_Q = self.Q.copy()\n",
    "            for t in range(len(S)-1):\n",
    "                St, At, Rt = S[t], A[t], R[t]\n",
    "\n",
    "                state_action_key = tuple(St.tolist() + list(At))\n",
    "\n",
    "                # Skip to next step if we have aldready encountered this state, action pair\n",
    "                if state_action_key in visited:\n",
    "                    continue\n",
    "\n",
    "                visited.add(state_action_key)\n",
    "\n",
    "                # Calculate return that follows the first occurence of St, At\n",
    "                G = 0\n",
    "                for dt in range(len(S)-2, t-1, -1):\n",
    "                    G = self.gamma * G + R[dt]\n",
    "\n",
    "                self.R[state_action_key].append(G)\n",
    "                self.Q[state_action_key] = np.average(self.R[state_action_key])\n",
    "\n",
    "            # Q-diff: Credit to Joakim Blach Andersen\n",
    "            Q_diff = abs(old_Q - self.Q)\n",
    "            print('Q-diff: {}'.format(np.max(Q_diff)))\n",
    "\n",
    "            # (c) Iterate over all states s and update the eps-greedy policy\n",
    "            for s in S:\n",
    "                y, x, y_vel, x_vel = s\n",
    "\n",
    "                possible_actions = self.track.possible_actions(s)\n",
    "                for a in possible_actions:\n",
    "                    a_y, a_x = a\n",
    "\n",
    "                    self.pi[y, x, y_vel, x_vel, a_y, a_x] = self.eps / \\\n",
    "                        len(possible_actions)\n",
    "\n",
    "                # Get index of best action\n",
    "                a_ys, a_xs = tuple(zip(*possible_actions))\n",
    "                actionvals = self.Q[y, x, y_vel,\n",
    "                                    x_vel, a_ys, a_xs]\n",
    "                a_max_idx = np.argmax(actionvals)\n",
    "                a_max_y, a_max_x = a_ys[a_max_idx], a_xs[a_max_idx]\n",
    "\n",
    "                self.pi[y, x, y_vel, x_vel, a_max_y, a_max_x] += 1 - self.eps\n",
    "\n",
    "                actionprobs = self.pi[y, x, y_vel, x_vel]\n",
    "                total_prob = np.sum(actionprobs)\n",
    "                if not math.isclose(total_prob, 1, abs_tol=0.01):\n",
    "                    print(\n",
    "                        'Action probabilities must sum to 1.0, but summed to {}, state: {}, actionprobs: {}'.format(total_prob, s, self.pi[y, x, y_vel, x_vel]))\n",
    "                    sys.exit(1)\n",
    "            \n",
    "            # Check if convergence in case no number of episodes is set to 0\n",
    "            if self.n_episodes is 0:\n",
    "                if np.allclose(old_Q, self.Q, rtol=0.05):\n",
    "                    print(\"Policy iteration converged after {} episodes\".format(it))\n",
    "                    policy_stable = True\n",
    "\n",
    "            # Counter and update epsilon\n",
    "            self.eps = 1/(1 + int(it/4))\n",
    "            it += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = OnPolicyMonteCarloAgent(rt, n_episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Step: 0\n",
      "Terminated after 3974 steps\n",
      "Q-diff: 3973.0\n",
      "Iteration 1\n",
      "Step: 0\n",
      "Terminated after 445 steps\n",
      "Q-diff: 1945.0\n",
      "Iteration 2\n",
      "Step: 0\n",
      "Terminated after 8051 steps\n",
      "Q-diff: 8036.0\n",
      "Iteration 3\n",
      "Step: 0\n",
      "Terminated after 4924 steps\n",
      "Q-diff: 4919.0\n",
      "Iteration 4\n",
      "Step: 0\n",
      "Terminated after 5185 steps\n",
      "Q-diff: 5156.0\n",
      "Iteration 5\n",
      "Step: 0\n",
      "Terminated after 6569 steps\n",
      "Q-diff: 6510.0\n",
      "Iteration 6\n",
      "Step: 0\n",
      "Terminated after 2020 steps\n",
      "Q-diff: 3435.5\n",
      "Iteration 7\n",
      "Step: 0\n",
      "Terminated after 5070 steps\n",
      "Q-diff: 5021.0\n",
      "Iteration 8\n",
      "Step: 0\n",
      "Terminated after 7022 steps\n",
      "Q-diff: 7017.0\n",
      "Iteration 9\n",
      "Step: 0\n",
      "Terminated after 754 steps\n",
      "Q-diff: 3771.0\n",
      "Iteration 10\n",
      "Step: 0\n",
      "Terminated after 5235 steps\n",
      "Q-diff: 5196.0\n"
     ]
    }
   ],
   "source": [
    "agent.policy_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
