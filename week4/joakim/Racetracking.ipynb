{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from itertools import permutations, repeat\n",
    "import random\n",
    "from numpy import random as numpy_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The velocity is also discrete, a number of grid cells \n",
    "# moved horizontally and vertically per time step.\n",
    "\n",
    "# Actions:\n",
    "# The actions are increments to the velocity components. \n",
    "# Each may be changed by +1, −1, or 0 in one step, \n",
    "# for a total of nine actions.\n",
    "\n",
    "# Both velocity components are restricted to be nonnegative \n",
    "# and less than 5, and they cannot both be zero except at the \n",
    "# starting line.\n",
    "\n",
    "# Episodes:\n",
    "# Each episode begins in one of the randomly selected \n",
    "# start states with both velocity components zero and \n",
    "# ends when the car crosses the finish line.\n",
    "\n",
    "# Rewards:\n",
    "# The rewards are −1 for each step until the car crosses \n",
    "# the finish line.\n",
    "\n",
    "# If the car hits the track boundary, it is moved back \n",
    "# to a random position on the starting line, both velocity \n",
    "# components are reduced to zero, and the episode continues.\n",
    "\n",
    "# With probability 0.1 at each time step the velocity increments are both zero, \n",
    "# independently of the intended increments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On-policy first-visit MC control (for ε-soft policies), estimates π ≈ π∗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load map1\n",
    "CELL_TYPE_WALL = 0  # Black boxes\n",
    "CELL_TYPE_TRACK = 1\n",
    "CELL_TYPE_GOAL = 2\n",
    "CELL_TYPE_START = 3\n",
    "\n",
    "class RaceTrack:\n",
    "    def __init__(self, track, zero_velocity_prob=0.9, max_vel=5, min_vel=0, gamma=0.9, epsilon=0.9):\n",
    "        self.track = track\n",
    "        self.wall_cells = np.argwhere(track == CELL_TYPE_WALL)\n",
    "        self.goal_cells = np.argwhere(track == CELL_TYPE_GOAL)\n",
    "        self.start_cells = np.argwhere(track == CELL_TYPE_START)\n",
    "        self.max_vel = max_vel\n",
    "        self.min_vel = min_vel\n",
    "        self.colors = ['black', 'white', 'yellow', 'red']  # For plotting\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.zero_velocity_prob = zero_velocity_prob\n",
    "        self.velocity_min = min_vel\n",
    "        self.velocity_max = max_vel\n",
    "        self.velocity_decrease_limit = -1\n",
    "        self.velocity_increase_limit = 1\n",
    "        \n",
    "        # Q-Matrix - a 6 dimensional vector for states, velocity, and action in both directions.\n",
    "        \n",
    "        # Q(s, a)\n",
    "        velocity_range = self.velocity_max - self.velocity_min + 1  # Minus for other direction.\n",
    "        velocity_change_range = self.velocity_increase_limit  - self.velocity_decrease_limit  + 1        \n",
    "        self.q = np.zeros((self.track.shape[0], self.track.shape[1], velocity_range, velocity_range, velocity_change_range, velocity_change_range))\n",
    "        \n",
    "        # Returns \n",
    "        self.Returns = defaultdict(list)\n",
    "\n",
    "        # Initialize policy\n",
    "        # NB: limit actions depending on action\n",
    "        self.pi_probabilities = np.zeros((self.track.shape[0], \n",
    "                                          self.track.shape[1], \n",
    "                                          velocity_range, \n",
    "                                          velocity_range, \n",
    "                                          velocity_change_range, \n",
    "                                          velocity_change_range), dtype=float)\n",
    "\n",
    "        # Initialize with equal probabilties for all possible actions        \n",
    "        for y_coord in range(self.pi_probabilities.shape[0]):\n",
    "            for x_coord in range(self.pi_probabilities.shape[1]):\n",
    "                for y_vel in range(self.velocity_min, self.velocity_max + 1):\n",
    "                    for x_vel in range(self.velocity_min, self.velocity_max + 1):\n",
    "                        possible_actions = self.possible_actions((y_vel, x_vel))\n",
    "                        for y_vel_change, x_vel_change in possible_actions:\n",
    "                            self.pi_probabilities[y_coord, x_coord, y_vel, x_vel, y_vel_change, x_vel_change] = 1/len(possible_actions)\n",
    "                                \n",
    "    def policy_iteration(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        policy_improvement = False\n",
    "        \n",
    "        k=0\n",
    "        while not policy_improvement:\n",
    "            print('Iteration {}'.format(k))\n",
    "            # Generate an episode\n",
    "            G = self.generate_episode()\n",
    "\n",
    "#            print('--EPSILON', self.epsilon)\n",
    "            \n",
    "#             print('Append G to Returns(s, a)')\n",
    "#             if '[0, 8, 0, 0, 1, 0]' in G.keys():            \n",
    "#                 print('G[0, 8, 0, 0, 1, 0]', G['[0, 8, 0, 0, 1, 0]'])\n",
    "#                 print('self.Returns[0, 8, 0, 0, 1, 0]', self.Returns['[0, 8, 0, 0, 1, 0]'])\n",
    "            for s_a in G.keys():\n",
    "                self.Returns[s_a].append(G[s_a])\n",
    "#             if '[0, 8, 0, 0, 1, 0]' in G.keys():            \n",
    "#                 print('self.Returns[0, 8, 0, 0, 1, 0]', self.Returns['[0, 8, 0, 0, 1, 0]'])\n",
    "            \n",
    "            print('Calculate averages in Q(s, a):')\n",
    "            print('Q-value  [0, 8, 0, 0, 1, 0]:', self.q[0, 8, 0, 0, 1, 0])\n",
    "            print('Q-value  [0, 8, 0, 0, 1, 1]:', self.q[0, 8, 0, 0, 1, 1])        \n",
    "            print('Q-value  [0, 8, 0, 0, 0, 1]:', self.q[0, 8, 0, 0, 0, 1])                \n",
    "            print('Q-value  [0, 8, 0, 0, 0, -1]:', self.q[0, 8, 0, 0, 0, -1])                \n",
    "\n",
    "            for s_a in self.Returns.keys():\n",
    "                self.q[eval(s_a)[0],\n",
    "                       eval(s_a)[1],\n",
    "                       eval(s_a)[2],\n",
    "                       eval(s_a)[3],\n",
    "                       eval(s_a)[4],\n",
    "                       eval(s_a)[5]] = np.average(self.Returns[s_a])\n",
    "            print('Q-value  [0, 8, 0, 0, 1, 0]:', self.q[0, 8, 0, 0, 1, 0])\n",
    "            print('Q-value  [0, 8, 0, 0, 1, 1]:', self.q[0, 8, 0, 0, 1, 1])        \n",
    "            print('Q-value  [0, 8, 0, 0, 0, 1]:', self.q[0, 8, 0, 0, 0, 1])                \n",
    "            print('Q-value  [0, 8, 0, 0, 0, -1]:', self.q[0, 8, 0, 0, 0, -1])                \n",
    "                        \n",
    "            # Old policy\n",
    "            old_policy = self.pi_probabilities.copy()\n",
    "\n",
    "            # Update pi(a | s)\n",
    "            self.update_policy()\n",
    "                \n",
    "            # Check if convergence\n",
    "            if np.allclose(old_policy, self.pi_probabilities, atol=0.0005):\n",
    "                print('Policy iteration converged.')\n",
    "                policy_improvement = True\n",
    "                \n",
    "            # Counter and update epsilon\n",
    "            self.epsilon = 1/(np.sqrt(k + 1.1))\n",
    "            \n",
    "            k += 1\n",
    "            \n",
    "    def update_policy(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        # Ranges\n",
    "        velocity_range = self.velocity_max - self.velocity_min + 1  # Minus for other direction.\n",
    "        velocity_change_range = self.velocity_increase_limit  - self.velocity_decrease_limit  + 1        \n",
    "\n",
    "        # Initialize policy\n",
    "        # NB: limit actions depending on action\n",
    "        self.pi_probabilities = np.zeros((rt.track.shape[0], \n",
    "                                          rt.track.shape[1], \n",
    "                                          velocity_range, \n",
    "                                          velocity_range, \n",
    "                                          velocity_change_range, \n",
    "                                          velocity_change_range), dtype=float)\n",
    "        \n",
    "        # Initialize with equal probabilties for all possible actions        \n",
    "        for y_coord in range(self.pi_probabilities.shape[0]):\n",
    "            for x_coord in range(self.pi_probabilities.shape[1]):\n",
    "                for y_vel in range(self.velocity_min, self.velocity_max + 1):\n",
    "                    for x_vel in range(self.velocity_min, self.velocity_max + 1):\n",
    "                        possible_actions = self.possible_actions((y_vel, x_vel))\n",
    "                        greedy_action = self.greedy_action(state=[y_coord, x_coord, y_vel, x_vel],\n",
    "                                                           possible_actions=possible_actions)\n",
    "                        for y_vel_change, x_vel_change in possible_actions:\n",
    "                            self.pi_probabilities[y_coord, \n",
    "                                                  x_coord, \n",
    "                                                  y_vel, \n",
    "                                                  x_vel, \n",
    "                                                  y_vel_change, \n",
    "                                                  x_vel_change] = self.epsilon_soft_policy(action=[y_vel_change, x_vel_change],\n",
    "                                                                                           greedy_action=greedy_action, \n",
    "                                                                                           all_state_actions=possible_actions)         \n",
    "            \n",
    "    def generate_episode(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        crossed_finishing_line = False\n",
    "        position = self.random_start_position()\n",
    "        first_occurence = defaultdict(int)\n",
    "        \n",
    "        step = 0\n",
    "        while not crossed_finishing_line:\n",
    "            step += 1\n",
    "            \n",
    "            # Sample action\n",
    "            action = self.sample_action_from_state(position)\n",
    "            \n",
    "            # Initiate s, a pair if not already in dict\n",
    "            if str(position + action) not in first_occurence.keys():                \n",
    "                first_occurence[str(position + action)] = step\n",
    "\n",
    "            # Old position\n",
    "            old_position = position.copy()\n",
    "            \n",
    "            # New position\n",
    "            position[0] += position[2] + action[0]\n",
    "            position[1] += position[3] + action[1]\n",
    "            position[2] += action[0]\n",
    "            position[3] += action[1]\n",
    "            \n",
    "            # Check if goal if reached (is it in the projected reactangle)\n",
    "            grid_states_to_check = self.get_all_grid_cells_in_projected_retcangle(current_state=[old_position[0], old_position[1]], \n",
    "                                                                                  new_state=[position[0], position[1]])\n",
    "            if self.check_if_goal_is_reached(check_grid_states=grid_states_to_check):\n",
    "                print('-- Goal Reached. Terminating Episode.')\n",
    "                break\n",
    "\n",
    "            # Check if car hits boundery\n",
    "            if position[0] >= self.track.shape[0] or position[1] >= self.track.shape[1]:\n",
    "                position = self.random_start_position()\n",
    "                continue\n",
    "            \n",
    "            new_grid_position = rt.track[position[0], position[1]]\n",
    "            if new_grid_position == 0:\n",
    "                position = self.random_start_position()\n",
    "                continue        \n",
    "\n",
    "        print('Steps {}'.format(step))\n",
    "\n",
    "        G = self._get_G_values(first_occurence_dict=first_occurence, total_steps=step)\n",
    "                \n",
    "        return G\n",
    "\n",
    "    def check_if_goal_is_reached(self, check_grid_states):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        grid_values = []\n",
    "\n",
    "        for y, x in check_grid_states:\n",
    "            if y <= self.track.shape[0] - 1 and x <= self.track.shape[1] - 1:\n",
    "                grid_values.append(rt.track[y, x])\n",
    "\n",
    "        if 2 in grid_values:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def get_all_grid_cells_in_projected_retcangle(self, current_state, new_state):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        y_coord_current = current_state[0]\n",
    "        x_coord_current = current_state[1]\n",
    "\n",
    "        y_diff = new_state[0] - current_state[0]\n",
    "        x_diff = new_state[1] - current_state[1]\n",
    "\n",
    "        return [[y_coord_current + y, x_coord_current + x] for y in range(0, y_diff + 1) for x in range(0, x_diff + 1)]\n",
    "\n",
    "\n",
    "    def sample_action_from_state(self, state):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        # Action coordinates in probability matrix\n",
    "        array = np.array(['(0, 0)', \n",
    "                          '(0, 1)', \n",
    "                          '(0, 2)', \n",
    "                          '(1, 0)', \n",
    "                          '(1, 1)', \n",
    "                          '(1, 2)',\n",
    "                          '(2, 0)',\n",
    "                          '(2, 1)',\n",
    "                          '(2, 2)'])\n",
    "        \n",
    "        # Randomly pick action\n",
    "        a = numpy_random.choice(array,\n",
    "                                size=1,\n",
    "                                p=rt.pi_probabilities[state[0], state[1], state[2], state[3]].flatten())        \n",
    "        a = eval(list(a)[0])        \n",
    "        \n",
    "        return self.center_axis_around_zero(coordinates=a, list_range=range(3))\n",
    "\n",
    "    def center_axis_around_zero(self, coordinates, list_range):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        return [x - len(list_range) if x > len(list_range)/2 else x for x in coordinates]\n",
    "\n",
    "    def greedy_action(self, state, possible_actions):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        greedy_action = possible_actions[0]\n",
    "        q_max = self.q[state[0], state[1], state[2], state[3], greedy_action[0], greedy_action[1]]\n",
    "        \n",
    "        for action in possible_actions:\n",
    "            value = self.q[state[0], state[1], state[2], state[3], action[0], action[1]]\n",
    "            if value > q_max:\n",
    "                q_max = value\n",
    "                greedy_action = action\n",
    "                    \n",
    "        return self.center_axis_around_zero(coordinates=greedy_action, list_range=range(3))\n",
    "\n",
    "    def epsilon_soft_policy(self, action, greedy_action, all_state_actions):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if action == greedy_action:\n",
    "            return 1 - self.epsilon + self.epsilon/len(all_state_actions)\n",
    "        \n",
    "        return self.epsilon/len(all_state_actions)\n",
    "\n",
    "    def random_start_position(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        grid_position = list(random.choice(np.argwhere(self.track==3)))\n",
    "        velocity = [0, 0]\n",
    "        \n",
    "        return  grid_position + velocity \n",
    "    \n",
    "    def possible_actions(self, velocity):\n",
    "        \"\"\"\n",
    "        Credit: Andreas\n",
    "        \"\"\"\n",
    "        actions = [[a_y, a_x] for a_y in range(-1, 2) for a_x in range(-1, 2)]\n",
    "        legal_actions = []\n",
    "        \n",
    "        v_y, v_x = velocity\n",
    "        \n",
    "        # Discard illegal actions\n",
    "        for a in actions:\n",
    "            a_y, a_x = a\n",
    "            # Cannot go above speed limit in any x direction\n",
    "            if v_x + a_x < self.min_vel or v_x + a_x > self.max_vel:\n",
    "                continue\n",
    "            # Cannot go above speed limit in any y direction\n",
    "            if v_y + a_y < self.min_vel or v_y + a_y > self.max_vel:\n",
    "                continue\n",
    "            # Cannot noop\n",
    "            if v_x + a_x == 0 and v_y + a_y == 0:\n",
    "                continue\n",
    "            legal_actions.append(a)\n",
    "            \n",
    "        return legal_actions\n",
    "\n",
    "    def _get_G_values(self, first_occurence_dict, total_steps):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        G = defaultdict(int)  # Dict. w/ G for first occurence for each s, a pair.\n",
    "\n",
    "        for key, val in first_occurence_dict.items():\n",
    "            number_rewards = total_steps - val\n",
    "\n",
    "            discounted_rewards = []\n",
    "            for k in range(number_rewards):\n",
    "                discounted_rewards.append(self.gamma**k * (-1))\n",
    "\n",
    "            G[key] = sum(discounted_rewards)\n",
    "\n",
    "        return G\n",
    "\n",
    "    @classmethod\n",
    "    def from_csv(cls, file_path):\n",
    "        \n",
    "        file_path = os.path.join(os.getcwd(), file_path)\n",
    "        \n",
    "        track = genfromtxt(file_path, delimiter=',')\n",
    "        track = np.flip(track, axis=0)\n",
    "        \n",
    "        return cls(track) \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 637\n"
     ]
    }
   ],
   "source": [
    "G = rt.generate_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt = RaceTrack.from_csv(\"../racetracks/map1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 1104\n",
      "--EPSILON 0.9\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.999999999999993\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.999999999999993\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.999999999999993\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 1\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 166\n",
      "--EPSILON 0.9534625892455922\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.999999999999993\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.999999999999993\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.999999999999993\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.999722333566368\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.874842224750335\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.999999806689011\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n"
     ]
    }
   ],
   "source": [
    "q = rt.policy_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-9.999999999999993, -9.749684449500677]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt.Returns['[0, 8, 0, 0, 1, 1]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -9.99999981,  0.        ],\n",
       "       [-9.99972233, -9.87484222,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q[0, 8, 0, 0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 613\n",
      "--EPSILON 0.9\n",
      "Append G to Returns(s, a)\n",
      "G[0, 8, 0, 0, 1, 0] -9.999999999999993\n",
      "self.Returns[0, 8, 0, 0, 1, 0] []\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993]\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.999999999999993\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.999999999999993\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.999999999999993\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 1\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 1896\n",
      "--EPSILON 0.9534625892455922\n",
      "Append G to Returns(s, a)\n",
      "G[0, 8, 0, 0, 1, 0] -9.999999999999993\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993]\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993]\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.999999999999993\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.999999999999993\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.999999999999993\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.999999999999993\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.999999999999993\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.999999999999993\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 2\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 125\n",
      "--EPSILON 0.6900655593423541\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.999999999999993\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.999999999999993\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.999999999999993\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.999999999999993\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.999993643875058\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.999969128709557\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 3\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 147\n",
      "--EPSILON 0.5679618342470648\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.999999999999993\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.999993643875058\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.999969128709557\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.999999999999993\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.999985266097596\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.999975350567652\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 4\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 962\n",
      "--EPSILON 0.49386479832479485\n",
      "Append G to Returns(s, a)\n",
      "G[0, 8, 0, 0, 1, 0] -9.999999999999993\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993]\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993]\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.999999999999993\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.999985266097596\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.999975350567652\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.999999999999993\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.999988212878076\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.999980280454121\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 5\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 805\n",
      "--EPSILON 0.4428074427700477\n",
      "Append G to Returns(s, a)\n",
      "G[0, 8, 0, 0, 1, 0] -9.999999999999993\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993]\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993]\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.999999999999993\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.999988212878076\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.999980280454121\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.999999999999993\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.999990177398395\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.9999835670451\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 6\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 500\n",
      "--EPSILON 0.404888165089458\n",
      "Append G to Returns(s, a)\n",
      "G[0, 8, 0, 0, 1, 0] -9.999999999999993\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993]\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993]\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.999999999999993\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.999990177398395\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.9999835670451\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.999999999999993\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.999991580627194\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.999985914610082\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 7\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 563\n",
      "--EPSILON 0.37529331252040077\n",
      "Append G to Returns(s, a)\n",
      "G[0, 8, 0, 0, 1, 0] -9.999999999999993\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993]\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993]\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.999999999999993\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.999991580627194\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.999985914610082\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.999999999999993\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.999992633048794\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.999987675283823\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 8\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 344\n",
      "--EPSILON 0.35136418446315326\n",
      "Append G to Returns(s, a)\n",
      "G[0, 8, 0, 0, 1, 0] -9.903022627021246\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993]\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246]\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.999999999999993\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.999992633048794\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.999987675283823\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.986146089574458\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.999992633048794\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.999989041270124\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 9\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 173\n",
      "--EPSILON 0.33149677206589795\n",
      "Append G to Returns(s, a)\n",
      "G[0, 8, 0, 0, 1, 0] -9.999999652040229\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246]\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229]\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.986146089574458\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.999992633048794\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.999989041270124\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.987877784882679\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.998932334986932\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.999989041270124\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 10\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 296\n",
      "--EPSILON 0.3146583877637763\n",
      "Append G to Returns(s, a)\n",
      "G[0, 8, 0, 0, 1, 0] -9.999999999997884\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229]\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884]\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.987877784882679\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.998932334986932\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.999989041270124\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.989224697673258\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.988263837823933\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.999989041270124\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 11\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 124\n",
      "--EPSILON 0.3001501125938321\n",
      "Append G to Returns(s, a)\n",
      "G[0, 8, 0, 0, 1, 0] -9.998838936929646\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884]\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884, -9.998838936929646]\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.989224697673258\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.988263837823933\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.999989041270124\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.990186121598896\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.989280276852007\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.999989041270124\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 12\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 475\n",
      "--EPSILON 0.2874797872880345\n",
      "Append G to Returns(s, a)\n",
      "G[0, 8, 0, 0, 1, 0] -9.999999999999993\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884, -9.998838936929646]\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884, -9.998838936929646, -9.999999999999993]\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.990186121598896\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.989280276852007\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.999989041270124\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.991078292362632\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.99017358711434\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.999985707450358\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 13\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 80\n",
      "--EPSILON 0.2762894819977688\n",
      "Append G to Returns(s, a)\n",
      "G[0, 8, 0, 0, 1, 0] -9.99303801390869\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884, -9.998838936929646, -9.999999999999993]\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884, -9.998838936929646, -9.999999999999993, -9.99303801390869]\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.991078292362632\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.99017358711434\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.999985707450358\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.99124160249147\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.990447481376144\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.947123609497561\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 14\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 600\n",
      "--EPSILON 0.26631182064565373\n",
      "Append G to Returns(s, a)\n",
      "G[0, 8, 0, 0, 1, 0] -9.999999999999993\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884, -9.998838936929646, -9.999999999999993, -9.99303801390869]\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884, -9.998838936929646, -9.999999999999993, -9.99303801390869, -9.999999999999993]\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.99124160249147\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.990447481376144\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.947123609497561\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.991915325376741\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.99112980413499\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.951529975372765\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 15\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 432\n",
      "--EPSILON 0.2573425063274894\n",
      "Append G to Returns(s, a)\n",
      "G[0, 8, 0, 0, 1, 0] -9.999999613378032\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884, -9.998838936929646, -9.999999999999993, -9.99303801390869, -9.999999999999993]\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884, -9.998838936929646, -9.999999999999993, -9.99303801390869, -9.999999999999993, -9.999999613378032]\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.991915325376741\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.99112980413499\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.951529975372765\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.992492774519691\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.99112980413499\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.955258438805629\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 16\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 246\n",
      "--EPSILON 0.2492223931396134\n",
      "Append G to Returns(s, a)\n",
      "G[0, 8, 0, 0, 1, 0] -9.999999999667699\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884, -9.998838936929646, -9.999999999999993, -9.99303801390869, -9.999999999999993, -9.999999613378032]\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884, -9.998838936929646, -9.999999999999993, -9.99303801390869, -9.999999999999993, -9.999999613378032, -9.999999999667699]\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.992492774519691\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.99112980413499\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.955258438805629\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.992993256196225\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.99112980413499\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.958454259999682\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 17\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 253\n",
      "--EPSILON 0.2418254167033372\n",
      "Append G to Returns(s, a)\n",
      "G[0, 8, 0, 0, 1, 0] -9.999998309981741\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884, -9.998838936929646, -9.999999999999993, -9.99303801390869, -9.999999999999993, -9.999999613378032, -9.999999999667699]\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884, -9.998838936929646, -9.999999999999993, -9.99303801390869, -9.999999999999993, -9.999999613378032, -9.999999999667699, -9.999998309981741]\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.992993256196225\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.99112980413499\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.958454259999682\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.993431072057819\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.991721096637292\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.961223975936168\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 18\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 144\n",
      "--EPSILON 0.23505024736113422\n",
      "Append G to Returns(s, a)\n",
      "G[0, 8, 0, 0, 1, 0] -9.903022627021246\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884, -9.998838936929646, -9.999999999999993, -9.99303801390869, -9.999999999999993, -9.999999613378032, -9.999999999667699, -9.999998309981741]\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884, -9.998838936929646, -9.999999999999993, -9.99303801390869, -9.999999999999993, -9.999999613378032, -9.999999999667699, -9.999998309981741, -9.903022627021246]\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.993431072057819\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.991721096637292\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.961223975936168\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.988112928232137\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.991721096637292\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.96361936371375\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 19\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 153\n",
      "--EPSILON 0.22881438097813775\n",
      "Append G to Returns(s, a)\n",
      "G[0, 8, 0, 0, 1, 0] -9.99959516233977\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884, -9.998838936929646, -9.999999999999993, -9.99303801390869, -9.999999999999993, -9.999999613378032, -9.999999999667699, -9.999998309981741, -9.903022627021246]\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884, -9.998838936929646, -9.999999999999993, -9.99303801390869, -9.999999999999993, -9.999999613378032, -9.999999999667699, -9.999998309981741, -9.903022627021246, -9.99959516233977]\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.988112928232137\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.991721096637292\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.96361936371375\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.988750830127007\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.991721096637292\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.96575914454044\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 20\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 54\n",
      "--EPSILON 0.22304986837273524\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.988750830127007\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.991721096637292\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.96575914454044\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.988750830127007\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.991721096637292\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.96575914454044\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 21\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 219\n",
      "--EPSILON 0.21770017209205406\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.988750830127007\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.991721096637292\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.96575914454044\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.988750830127007\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.991721096637292\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.967661414119473\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 22\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 112\n",
      "--EPSILON 0.21271781490575853\n",
      "Append G to Returns(s, a)\n",
      "G[0, 8, 0, 0, 1, 0] -9.983826907300768\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884, -9.998838936929646, -9.999999999999993, -9.99303801390869, -9.999999999999993, -9.999999613378032, -9.999999999667699, -9.999998309981741, -9.903022627021246, -9.99959516233977]\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884, -9.998838936929646, -9.999999999999993, -9.99303801390869, -9.999999999999993, -9.999999613378032, -9.999999999667699, -9.999998309981741, -9.903022627021246, -9.99959516233977, -9.983826907300768]\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.988750830127007\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.991721096637292\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.967661414119473\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.988491676294046\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.98617744228629\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.969205713022196\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 23\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 32\n",
      "--EPSILON 0.20806259464411975\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.988491676294046\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.98617744228629\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.969205713022196\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.988491676294046\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.98617744228629\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.969205713022196\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 24\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 16\n",
      "--EPSILON 0.20370021093167762\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.988491676294046\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.98617744228629\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.969205713022196\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.988491676294046\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.98617744228629\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.969205713022196\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 25\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 112\n",
      "--EPSILON 0.1996011960139498\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.988491676294046\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.98617744228629\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.969205713022196\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.988491676294046\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.98617744228629\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.970665793415645\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 26\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 92\n",
      "--EPSILON 0.19574007317156783\n",
      "Append G to Returns(s, a)\n",
      "G[0, 8, 0, 0, 1, 0] -9.972610725500465\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884, -9.998838936929646, -9.999999999999993, -9.99303801390869, -9.999999999999993, -9.999999613378032, -9.999999999667699, -9.999998309981741, -9.903022627021246, -9.99959516233977, -9.983826907300768]\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884, -9.998838936929646, -9.999999999999993, -9.99303801390869, -9.999999999999993, -9.999999613378032, -9.999999999667699, -9.999998309981741, -9.903022627021246, -9.99959516233977, -9.983826907300768, -9.972610725500465]\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.988491676294046\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.98617744228629\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.970665793415645\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.987697628754367\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.98617744228629\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.971986818533525\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 27\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 25\n",
      "--EPSILON 0.19209468759882464\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.987697628754367\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.98617744228629\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.971986818533525\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.987697628754367\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.92906283378806\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.971986818533525\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 28\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 94\n",
      "--EPSILON 0.18864566947613623\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.987697628754367\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.92906283378806\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.971986818533525\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.987697628754367\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.932348777212184\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.973149804306987\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 29\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 320\n",
      "--EPSILON 0.18537599944001618\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.987697628754367\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.932348777212184\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.973149804306987\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.987697628754367\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.935909367883676\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.966383289497378\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 30\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 32\n",
      "--EPSILON 0.1822706541441223\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.987697628754367\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.935909367883676\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.966383289497378\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.987697628754367\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.935909367883676\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.966383289497378\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 31\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 46\n",
      "--EPSILON 0.17931631503020815\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.987697628754367\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.935909367883676\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.966383289497378\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.987697628754367\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.846462890046901\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.966383289497378\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 32\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 838\n",
      "--EPSILON 0.17650112740455193\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.987697628754367\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.846462890046901\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.966383289497378\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.987697628754367\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.853774180997048\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.96778398576832\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 33\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 85\n",
      "--EPSILON 0.17381449986274955\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.987697628754367\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.853774180997048\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.96778398576832\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.987697628754367\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.860340370794715\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.96778398576832\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 34\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 88\n",
      "--EPSILON 0.17124693631268542\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.987697628754367\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.860340370794715\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.96778398576832\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.987697628754367\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.862617761817516\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.96778398576832\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 35\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 193\n",
      "--EPSILON 0.16878989451394444\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.987697628754367\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.862617761817516\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.96778398576832\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.987697628754367\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.868342018056504\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.96907262574757\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 36\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 126\n",
      "--EPSILON 0.16643566632465154\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.987697628754367\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.868342018056504\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.96907262574757\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.987697628754367\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.873603255640345\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.969994371446075\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 37\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 651\n",
      "--EPSILON 0.16417727582577962\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.987697628754367\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.873603255640345\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.969994371446075\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.987697628754367\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.878464668884947\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.971105691022148\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 38\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 179\n",
      "--EPSILON 0.16200839225208363\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.987697628754367\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.878464668884947\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.971105691022148\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.987697628754367\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.88296597189716\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972137628325726\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 39\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 100\n",
      "--EPSILON 0.15992325525180032\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.987697628754367\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.88296597189716\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972137628325726\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.987697628754367\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.887136272401232\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972137628325726\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 40\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 137\n",
      "--EPSILON 0.15791661046371636\n",
      "Append G to Returns(s, a)\n",
      "G[0, 8, 0, 0, 1, 0] -9.202335569231275\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884, -9.998838936929646, -9.999999999999993, -9.99303801390869, -9.999999999999993, -9.999999613378032, -9.999999999667699, -9.999998309981741, -9.903022627021246, -9.99959516233977, -9.983826907300768, -9.972610725500465]\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884, -9.998838936929646, -9.999999999999993, -9.99303801390869, -9.999999999999993, -9.999999613378032, -9.999999999667699, -9.999998309981741, -9.903022627021246, -9.99959516233977, -9.983826907300768, -9.972610725500465, -9.202335569231275]\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.987697628754367\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.887136272401232\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972137628325726\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.950299435443744\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.891027645737559\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972137628325726\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 41\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 31\n",
      "--EPSILON 0.15598365376958254\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.950299435443744\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.891027645737559\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972137628325726\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.950299435443744\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.891027645737559\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972137628325726\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 42\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 278\n",
      "--EPSILON 0.15411998287418846\n",
      "Append G to Returns(s, a)\n",
      "G[0, 8, 0, 0, 1, 0] -9.999999999630775\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884, -9.998838936929646, -9.999999999999993, -9.99303801390869, -9.999999999999993, -9.999999613378032, -9.999999999667699, -9.999998309981741, -9.903022627021246, -9.99959516233977, -9.983826907300768, -9.972610725500465, -9.202335569231275]\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884, -9.998838936929646, -9.999999999999993, -9.99303801390869, -9.999999999999993, -9.999999613378032, -9.999999999667699, -9.999998309981741, -9.903022627021246, -9.99959516233977, -9.983826907300768, -9.972610725500465, -9.202335569231275, -9.999999999630775]\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.950299435443744\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.891027645737559\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972137628325726\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.952558551997699\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.894660057545927\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972137628325726\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 43\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 109\n",
      "--EPSILON 0.15232155510134432\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.952558551997699\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.894660057545927\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972137628325726\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.952558551997699\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.898041997638622\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972137628325726\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 44\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 15\n",
      "--EPSILON 0.15058465048420852\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.952558551997699\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.898041997638622\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972137628325726\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.952558551997699\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.836887206432838\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972137628325726\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 45\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 19\n",
      "--EPSILON 0.14890583938253493\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.952558551997699\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.836887206432838\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972137628325726\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.952558551997699\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.836887206432838\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972137628325726\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 46\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 41\n",
      "--EPSILON 0.1472819539849714\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.952558551997699\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.836887206432838\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972137628325726\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.952558551997699\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.764803781137026\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972137628325726\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 47\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 63\n",
      "--EPSILON 0.145710063157312\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.952558551997699\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.764803781137026\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972137628325726\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.952558551997699\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.770915750088891\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972137628325726\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 48\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 181\n",
      "--EPSILON 0.1441874501821181\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.952558551997699\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.770915750088891\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972137628325726\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.952558551997699\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.777460469561358\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972137628325726\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 49\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 350\n",
      "--EPSILON 0.1427115930049275\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.952558551997699\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.777460469561358\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972137628325726\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.952558551997699\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.78364212315524\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972137628325726\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 50\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 119\n",
      "--EPSILON 0.14128014666017077\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.952558551997699\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.78364212315524\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972137628325726\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.952558551997699\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.78364212315524\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972137628325726\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 51\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 20\n",
      "--EPSILON 0.13989092759813318\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.952558551997699\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.78364212315524\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972137628325726\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.952558551997699\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.78364212315524\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972137628325726\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 52\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 503\n",
      "--EPSILON 0.1385418996746149\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.952558551997699\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.78364212315524\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972137628325726\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.952558551997699\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.789489633340231\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.973098399762769\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 53\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 834\n",
      "--EPSILON 0.1372311615987697\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.952558551997699\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.789489633340231\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.973098399762769\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.952558551997699\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.795029379831279\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.973995042495037\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 54\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 145\n",
      "--EPSILON 0.13595693566309272\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.952558551997699\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.795029379831279\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.973995042495037\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.952558551997699\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.800284208746882\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.973995042495037\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 55\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 115\n",
      "--EPSILON 0.13471755760359808\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.952558551997699\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.800284208746882\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.973995042495037\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.952558551997699\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.800284208746882\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.973995042495037\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 56\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 122\n",
      "--EPSILON 0.13351146745863818\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.952558551997699\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.800284208746882\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.973995042495037\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.952558551997699\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.80527644960589\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.973995042495037\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 57\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 798\n",
      "--EPSILON 0.13233720131217205\n",
      "Append G to Returns(s, a)\n",
      "G[0, 8, 0, 0, 1, 0] -9.999999999999993\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884, -9.998838936929646, -9.999999999999993, -9.99303801390869, -9.999999999999993, -9.999999613378032, -9.999999999667699, -9.999998309981741, -9.903022627021246, -9.99959516233977, -9.983826907300768, -9.972610725500465, -9.202335569231275, -9.999999999630775]\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884, -9.998838936929646, -9.999999999999993, -9.99303801390869, -9.999999999999993, -9.999999613378032, -9.999999999667699, -9.999998309981741, -9.903022627021246, -9.99959516233977, -9.983826907300768, -9.972610725500465, -9.202335569231275, -9.999999999630775, -9.999999999999993]\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.952558551997699\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.80527644960589\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.973995042495037\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.954621223649974\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.81002580449355\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.973995042495037\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 58\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 54\n",
      "--EPSILON 0.13119338382209642\n",
      "Append G to Returns(s, a)\n",
      "G[0, 8, 0, 0, 1, 0] -9.202335569231275\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884, -9.998838936929646, -9.999999999999993, -9.99303801390869, -9.999999999999993, -9.999999613378032, -9.999999999667699, -9.999998309981741, -9.903022627021246, -9.99959516233977, -9.983826907300768, -9.972610725500465, -9.202335569231275, -9.999999999630775, -9.999999999999993]\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884, -9.998838936929646, -9.999999999999993, -9.99303801390869, -9.999999999999993, -9.999999613378032, -9.999999999667699, -9.999998309981741, -9.903022627021246, -9.99959516233977, -9.983826907300768, -9.972610725500465, -9.202335569231275, -9.999999999630775, -9.999999999999993, -9.202335569231275]\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.954621223649974\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.81002580449355\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.973995042495037\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.923275988049193\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.81002580449355\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.973995042495037\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 59\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 233\n",
      "--EPSILON 0.13007872144692093\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.923275988049193\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.81002580449355\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.973995042495037\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.923275988049193\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.814548999308576\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.973995042495037\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 60\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 95\n",
      "--EPSILON 0.12899199629493716\n",
      "Append G to Returns(s, a)\n",
      "G[0, 8, 0, 0, 1, 0] -9.99588901683294\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884, -9.998838936929646, -9.999999999999993, -9.99303801390869, -9.999999999999993, -9.999999613378032, -9.999999999667699, -9.999998309981741, -9.903022627021246, -9.99959516233977, -9.983826907300768, -9.972610725500465, -9.202335569231275, -9.999999999630775, -9.999999999999993, -9.202335569231275]\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884, -9.998838936929646, -9.999999999999993, -9.99303801390869, -9.999999999999993, -9.999999613378032, -9.999999999667699, -9.999998309981741, -9.903022627021246, -9.99959516233977, -9.983826907300768, -9.972610725500465, -9.202335569231275, -9.999999999630775, -9.999999999999993, -9.202335569231275, -9.99588901683294]\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.923275988049193\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.814548999308576\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.973995042495037\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.926180509200544\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.817890985855371\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.973995042495037\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 61\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 139\n",
      "--EPSILON 0.12793206052938005\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.926180509200544\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.817890985855371\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.973995042495037\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.926180509200544\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.822012514052282\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.971705609737818\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 62\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 296\n",
      "--EPSILON 0.12689783127114684\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.926180509200544\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.822012514052282\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.971705609737818\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.926180509200544\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.825967791517781\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972474188031935\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 63\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 30\n",
      "--EPSILON 0.12588828594761015\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.926180509200544\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.825967791517781\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972474188031935\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.926180509200544\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.825967791517781\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972474188031935\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 64\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 62\n",
      "--EPSILON 0.1249024580421098\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.926180509200544\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.825967791517781\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972474188031935\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.926180509200544\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.825967791517781\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972474188031935\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 65\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 17\n",
      "--EPSILON 0.12393943320395891\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.926180509200544\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.825967791517781\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972474188031935\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.926180509200544\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.825967791517781\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972474188031935\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 66\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 55\n",
      "--EPSILON 0.12299834568337578\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.926180509200544\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.825967791517781\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972474188031935\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.926180509200544\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.825967791517781\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972474188031935\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 67\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 61\n",
      "--EPSILON 0.12207837505974602\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.926180509200544\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.825967791517781\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972474188031935\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.926180509200544\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.825967791517781\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972474188031935\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 68\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 89\n",
      "--EPSILON 0.12117874323511166\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.926180509200544\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.825967791517781\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.972474188031935\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.926180509200544\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.829494792037444\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.971902728489578\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 69\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 42\n",
      "--EPSILON 0.12029871166784606\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.926180509200544\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.829494792037444\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.971902728489578\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.926180509200544\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.829494792037444\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.971902728489578\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 70\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 165\n",
      "--EPSILON 0.1194375788241625\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.926180509200544\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.829494792037444\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.971902728489578\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.926180509200544\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.833122551135677\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.971902728489578\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 71\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 113\n",
      "--EPSILON 0.11859467782747\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.926180509200544\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.833122551135677\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.971902728489578\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.926180509200544\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.836597758080513\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.971902728489578\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 72\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 47\n",
      "--EPSILON 0.117769374287677\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.926180509200544\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.836597758080513\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.971902728489578\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.926180509200544\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.838489711897207\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.971902728489578\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 73\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 255\n",
      "--EPSILON 0.11696106429438609\n",
      "Append G to Returns(s, a)\n",
      "G[0, 8, 0, 0, 1, 0] -9.994360791266038\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884, -9.998838936929646, -9.999999999999993, -9.99303801390869, -9.999999999999993, -9.999999613378032, -9.999999999667699, -9.999998309981741, -9.903022627021246, -9.99959516233977, -9.983826907300768, -9.972610725500465, -9.202335569231275, -9.999999999630775, -9.999999999999993, -9.202335569231275, -9.99588901683294]\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884, -9.998838936929646, -9.999999999999993, -9.99303801390869, -9.999999999999993, -9.999999613378032, -9.999999999667699, -9.999998309981741, -9.903022627021246, -9.99959516233977, -9.983826907300768, -9.972610725500465, -9.202335569231275, -9.999999999630775, -9.999999999999993, -9.202335569231275, -9.99588901683294, -9.994360791266038]\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.926180509200544\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.838489711897207\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.971902728489578\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.841719917656945\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.961507929872916\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 74\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 212\n",
      "--EPSILON 0.11616917255955382\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.841719917656945\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.961507929872916\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.844817018908676\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 75\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 24\n",
      "--EPSILON 0.1153931506966354\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.844817018908676\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.83246160641488\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 76\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 37\n",
      "--EPSILON 0.11463247562451714\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.83246160641488\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.83246160641488\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 77\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 13\n",
      "--EPSILON 0.11388664808568191\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.83246160641488\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.83246160641488\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 78\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 52\n",
      "--EPSILON 0.1131551912690689\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.83246160641488\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.8284242096056\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 79\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 209\n",
      "--EPSILON 0.11243764952899701\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.8284242096056\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.831601528144885\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 80\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 14\n",
      "--EPSILON 0.11173358719233181\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.831601528144885\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.831601528144885\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 81\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 168\n",
      "--EPSILON 0.11104258744680078\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.831601528144885\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.834663314806491\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 82\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 92\n",
      "--EPSILON 0.1103642513040126\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.834663314806491\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.837604737104247\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 83\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 24\n",
      "--EPSILON 0.1096981966313196\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.837604737104247\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.837604737104247\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 84\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 58\n",
      "--EPSILON 0.10904405724718551\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.837604737104247\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.837604737104247\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 85\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 30\n",
      "--EPSILON 0.10840148207519447\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.837604737104247\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.837604737104247\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 86\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 30\n",
      "--EPSILON 0.10777013435226059\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.837604737104247\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.807944650683966\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 87\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 26\n",
      "--EPSILON 0.10714969088698138\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.807944650683966\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.807944650683966\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 88\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 92\n",
      "--EPSILON 0.10653984136442511\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.807944650683966\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.807944650683966\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 89\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 108\n",
      "--EPSILON 0.10594028769395471\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.807944650683966\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.811253978463332\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 90\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 60\n",
      "--EPSILON 0.1053507433969748\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.811253978463332\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.814148485557189\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 91\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 36\n",
      "--EPSILON 0.10477093303174541\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.814148485557189\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.813491277540411\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 92\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 92\n",
      "--EPSILON 0.1042005916526391\n",
      "Append G to Returns(s, a)\n",
      "G[0, 8, 0, 0, 1, 0] -9.89224736335694\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884, -9.998838936929646, -9.999999999999993, -9.99303801390869, -9.999999999999993, -9.999999613378032, -9.999999999667699, -9.999998309981741, -9.903022627021246, -9.99959516233977, -9.983826907300768, -9.972610725500465, -9.202335569231275, -9.999999999630775, -9.999999999999993, -9.202335569231275, -9.99588901683294, -9.994360791266038]\n",
      "self.Returns[0, 8, 0, 0, 1, 0] [-9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.999999999999993, -9.903022627021246, -9.999999652040229, -9.999999999997884, -9.998838936929646, -9.999999999999993, -9.99303801390869, -9.999999999999993, -9.999999613378032, -9.999999999667699, -9.999998309981741, -9.903022627021246, -9.99959516233977, -9.983826907300768, -9.972610725500465, -9.202335569231275, -9.999999999630775, -9.999999999999993, -9.202335569231275, -9.99588901683294, -9.994360791266038, -9.89224736335694]\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.928802827741523\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.813491277540411\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.814959004581082\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 93\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 90\n",
      "--EPSILON 0.10363946430143024\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.814959004581082\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.817931250096217\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 94\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 58\n",
      "--EPSILON 0.10308730552839766\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.817931250096217\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.820469082438427\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 95\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 23\n",
      "--EPSILON 0.10254387894119787\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.820469082438427\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.820469082438427\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 96\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 11\n",
      "--EPSILON 0.10200895677962514\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.820469082438427\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.820469082438427\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 97\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 12\n",
      "--EPSILON 0.10148231951452123\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.820469082438427\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.779144637950171\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 98\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 22\n",
      "--EPSILON 0.10096375546923043\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.779144637950171\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.779144637950171\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 99\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 47\n",
      "--EPSILON 0.10045306046211759\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.779144637950171\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.779144637950171\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 100\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 50\n",
      "--EPSILON 0.09995003746877731\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.779144637950171\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.779144637950171\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 101\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 14\n",
      "--EPSILON 0.09945449630266603\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.779144637950171\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.739091714830783\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 102\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 15\n",
      "--EPSILON 0.09896625331298045\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.739091714830783\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.739091714830783\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 103\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 11\n",
      "--EPSILON 0.09848513109869263\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.739091714830783\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.739091714830783\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 104\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 25\n",
      "--EPSILON 0.09801095823773072\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.739091714830783\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.714968807199229\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 105\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 11\n",
      "--EPSILON 0.09754356903036576\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.714968807199229\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.714968807199229\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 106\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 24\n",
      "--EPSILON 0.09708280325593283\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.714968807199229\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.714968807199229\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 107\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 22\n",
      "--EPSILON 0.09662850594207474\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.714968807199229\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.701077234764828\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 108\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 36\n",
      "--EPSILON 0.09618052714575416\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.701077234764828\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.702160157849912\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 109\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 37\n",
      "--EPSILON 0.09573872174533082\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.702160157849912\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.702160157849912\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 110\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 27\n",
      "--EPSILON 0.09530294924304941\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.702160157849912\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.693631839446793\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 111\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 49\n",
      "--EPSILON 0.09487307357732752\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.693631839446793\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.693631839446793\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 112\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 16\n",
      "--EPSILON 0.09444896294427423\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.693631839446793\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.693631839446793\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 113\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 20\n",
      "--EPSILON 0.09403048962790747\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.693631839446793\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.657661450814553\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 114\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 24\n",
      "--EPSILON 0.09361752983857377\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.657661450814553\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.651248410228872\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 115\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 11\n",
      "--EPSILON 0.09320996355910582\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.651248410228872\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.651248410228872\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 116\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 20\n",
      "--EPSILON 0.09280767439828409\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.651248410228872\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.651248410228872\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 117\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 31\n",
      "--EPSILON 0.0924105494511958\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.651248410228872\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.620788490248904\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 118\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 12\n",
      "--EPSILON 0.09201847916611114\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.620788490248904\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.587294190864535\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 119\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 55\n",
      "--EPSILON 0.09163135721752008\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.587294190864535\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.590873879779684\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 120\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 39\n",
      "--EPSILON 0.09124908038499573\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.590873879779684\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.585693368972374\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 121\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 13\n",
      "--EPSILON 0.09087154843757085\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.585693368972374\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.585693368972374\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 122\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 28\n",
      "--EPSILON 0.09049866402333347\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.585693368972374\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.585693368972374\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 123\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 24\n",
      "--EPSILON 0.0901303325639653\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.585693368972374\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.585693368972374\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 124\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 11\n",
      "--EPSILON 0.08976646215396372\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.585693368972374\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.585693368972374\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 125\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 14\n",
      "--EPSILON 0.08940696346430335\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.585693368972374\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.585693368972374\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 126\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 23\n",
      "--EPSILON 0.08905174965030797\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.585693368972374\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.585693368972374\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 127\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 110\n",
      "--EPSILON 0.08870073626351706\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.585693368972374\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.93981729099744\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.59114355340864\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.9387952169981\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 128\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 11\n",
      "--EPSILON 0.08835384116734371\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.59114355340864\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.9387952169981\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.59114355340864\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.9387952169981\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 129\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 39\n",
      "--EPSILON 0.088010984456333\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.59114355340864\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.9387952169981\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.591498566682855\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.9387952169981\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 130\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 21\n",
      "--EPSILON 0.0876720883788401\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.591498566682855\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.9387952169981\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.591498566682855\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.9387952169981\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 131\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 59\n",
      "--EPSILON 0.08733707726295863\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.591498566682855\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.9387952169981\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.58114901395736\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.9387952169981\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 132\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 11\n",
      "--EPSILON 0.08700587744553888\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.58114901395736\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.9387952169981\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.58114901395736\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.9387952169981\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 133\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 27\n",
      "--EPSILON 0.08667841720414475\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.58114901395736\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.9387952169981\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.579090198970173\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.9387952169981\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 134\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 42\n",
      "--EPSILON 0.08635462669180663\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.579090198970173\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.9387952169981\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.54904787942292\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.9387952169981\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 135\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 15\n",
      "--EPSILON 0.08603443787443547\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.54904787942292\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.9387952169981\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.529196531270212\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.9387952169981\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 136\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 15\n",
      "--EPSILON 0.0857177844707708\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.529196531270212\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.9387952169981\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.529196531270212\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.9387952169981\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 137\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 17\n",
      "--EPSILON 0.08540460189474228\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.529196531270212\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.9387952169981\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.529196531270212\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.9387952169981\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 138\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 28\n",
      "--EPSILON 0.08509482720013087\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.529196531270212\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.9387952169981\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.529196531270212\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.9387952169981\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Iteration 139\n",
      "-- Goal Reached. Terminating Episode.\n",
      "Steps 11\n",
      "--EPSILON 0.08478839902742212\n",
      "Append G to Returns(s, a)\n",
      "Calculate averages in Q(s, a):\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.529196531270212\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.9387952169981\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Q-value  [0, 8, 0, 0, 1, 0]: -9.927448921653205\n",
      "Q-value  [0, 8, 0, 0, 1, 1]: -9.529196531270212\n",
      "Q-value  [0, 8, 0, 0, 0, 1]: -9.9387952169981\n",
      "Q-value  [0, 8, 0, 0, 0, -1]: 0.0\n",
      "Policy iteration converged.\n"
     ]
    }
   ],
   "source": [
    "rt.policy_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"On_Policy_Q_matrix_Joakim.csv\", rt.q, allow_pickle=True, fix_imports=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.93090702, 0.        ],\n",
       "       [0.03454649, 0.03454649, 0.        ],\n",
       "       [0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt.pi_probabilities[0, 8, 0, 0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Off-policy MC prediction, for estimating Q ≈ qπ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load map1\n",
    "CELL_TYPE_WALL = 0  # Black boxes\n",
    "CELL_TYPE_TRACK = 1\n",
    "CELL_TYPE_GOAL = 2\n",
    "CELL_TYPE_START = 3\n",
    "\n",
    "class RaceTrack:\n",
    "    def __init__(self, track, zero_velocity_prob=0.9, max_vel=5, min_vel=0, gamma=0.25, epsilon=0.9):\n",
    "        self.track = track\n",
    "        self.wall_cells = np.argwhere(track == CELL_TYPE_WALL)\n",
    "        self.goal_cells = np.argwhere(track == CELL_TYPE_GOAL)\n",
    "        self.start_cells = np.argwhere(track == CELL_TYPE_START)\n",
    "        self.max_vel = max_vel\n",
    "        self.min_vel = min_vel\n",
    "        self.colors = ['black', 'white', 'yellow', 'red']  # For plotting\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.zero_velocity_prob = zero_velocity_prob\n",
    "        self.velocity_min = min_vel\n",
    "        self.velocity_max = max_vel\n",
    "        self.velocity_decrease_limit = -1\n",
    "        self.velocity_increase_limit = 1\n",
    "        \n",
    "        # Q-Matrix - a 6 dimensional vector for states, velocity, and action in both directions.\n",
    "        \n",
    "        # Q(s, a)\n",
    "        velocity_range = self.velocity_max - self.velocity_min + 1  # Minus for other direction.\n",
    "        velocity_change_range = self.velocity_increase_limit  - self.velocity_decrease_limit  + 1        \n",
    "        self.q = np.zeros((self.track.shape[0], self.track.shape[1], velocity_range, velocity_range, velocity_change_range, velocity_change_range))\n",
    "        \n",
    "        # Returns \n",
    "        self.Returns = defaultdict(list)\n",
    "\n",
    "        # Initialize policy\n",
    "        # NB: limit actions depending on action\n",
    "        self.pi_probabilities = np.zeros((self.track.shape[0], \n",
    "                                          self.track.shape[1], \n",
    "                                          velocity_range, \n",
    "                                          velocity_range, \n",
    "                                          velocity_change_range, \n",
    "                                          velocity_change_range), dtype=float)\n",
    "\n",
    "        # Initialize with equal probabilties for all possible actions        \n",
    "        for y_coord in range(self.pi_probabilities.shape[0]):\n",
    "            for x_coord in range(self.pi_probabilities.shape[1]):\n",
    "                for y_vel in range(self.velocity_min, self.velocity_max + 1):\n",
    "                    for x_vel in range(self.velocity_min, self.velocity_max + 1):\n",
    "                        possible_actions = self.possible_actions((y_vel, x_vel))\n",
    "                        for y_vel_change, x_vel_change in possible_actions:\n",
    "                            self.pi_probabilities[y_coord, x_coord, y_vel, x_vel, y_vel_change, x_vel_change] = 1/len(possible_actions)\n",
    "                                \n",
    "    def policy_iteration(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        policy_improvement = False\n",
    "        \n",
    "        k=0\n",
    "        while not policy_improvement:\n",
    "            print('Iteration {}'.format(k))\n",
    "            # Generate an episode\n",
    "            G = self.generate_episode()\n",
    "\n",
    "            print('--EPSILON', self.epsilon)\n",
    "            \n",
    "            # Append G to Returns(s, a)\n",
    "#             if '[0, 8, 0, 0, 1, 0]' in G.keys():            \n",
    "#                 print('G[0, 8, 0, 0, 1, 0]', G['[0, 8, 0, 0, 1, 0]'])\n",
    "#                 print('self.Returns[0, 8, 0, 0, 1, 0]', self.Returns['[0, 8, 0, 0, 1, 0]'])\n",
    "            for s_a in G.keys():\n",
    "                self.Returns[s_a].append(G[s_a])\n",
    "#             if '[0, 8, 0, 0, 1, 0]' in G.keys():            \n",
    "#                 print('self.Returns[0, 8, 0, 0, 1, 0]', self.Returns['[0, 8, 0, 0, 1, 0]'])\n",
    "            \n",
    "            # Old Q(s, a)\n",
    "            old_q = self.q.copy()\n",
    "            \n",
    "            # Calculate averages in Q(s, a)\n",
    "#             print('Q-value  [0, 8, 0, 0, 1, 0]:', self.q[0, 8, 0, 0, 1, 0])\n",
    "#             print('Q-value  [0, 8, 0, 0, 1, 1]:', self.q[0, 8, 0, 0, 1, 1])        \n",
    "#             print('Q-value  [0, 8, 0, 0, 0, 1]:', self.q[0, 8, 0, 0, 0, 1])                \n",
    "#             print('Q-value  [0, 8, 0, 0, 0, -1]:', self.q[0, 8, 0, 0, 0, -1])                \n",
    "            for s_a in self.Returns.keys():\n",
    "                self.q[eval(s_a)[0],\n",
    "                       eval(s_a)[1],\n",
    "                       eval(s_a)[2],\n",
    "                       eval(s_a)[3],\n",
    "                       eval(s_a)[4],\n",
    "                       eval(s_a)[5]] = np.average(self.Returns[s_a])\n",
    "#             print('Q-value  [0, 8, 0, 0, 1, 0]:', self.q[0, 8, 0, 0, 1, 0])\n",
    "#             print('Q-value  [0, 8, 0, 0, 1, 1]:', self.q[0, 8, 0, 0, 1, 1])        \n",
    "#             print('Q-value  [0, 8, 0, 0, 0, 1]:', self.q[0, 8, 0, 0, 0, 1])                \n",
    "#             print('Q-value  [0, 8, 0, 0, 0, -1]:', self.q[0, 8, 0, 0, 0, -1])                \n",
    "            \n",
    "            # Q-diff\n",
    "            q_diff = abs(old_q - self.q)\n",
    "#            print('Q-diff: {}'.format(np.max(q_diff)))\n",
    "            \n",
    "            # Old policy\n",
    "            old_policy = self.pi_probabilities.copy()\n",
    "\n",
    "#             print('Pi Prob  [0, 8, 0, 0, 1, 0]:', self.pi_probabilities[0, 8, 0, 0, 1, 0])\n",
    "#             print('Pi Prob  [0, 8, 0, 0, 1, 1]:', self.pi_probabilities[0, 8, 0, 0, 1, 1])        \n",
    "#             print('Pi Prob  [0, 8, 0, 0, 0, 1]:', self.pi_probabilities[0, 8, 0, 0, 0, 1])                \n",
    "#             print('Pi Prob  [0, 8, 0, 0, 0, -1]:', self.pi_probabilities[0, 8, 0, 0, 0, -1])            \n",
    "            # Update pi(a | s)\n",
    "            self.update_policy()\n",
    "#             print('Pi Prob  [0, 8, 0, 0, 1, 0]:', self.pi_probabilities[0, 8, 0, 0, 1, 0])\n",
    "#             print('Pi Prob  [0, 8, 0, 0, 1, 1]:', self.pi_probabilities[0, 8, 0, 0, 1, 1])        \n",
    "#             print('Pi Prob  [0, 8, 0, 0, 0, 1]:', self.pi_probabilities[0, 8, 0, 0, 0, 1])                \n",
    "#             print('Pi Prob  [0, 8, 0, 0, 0, -1]:', self.pi_probabilities[0, 8, 0, 0, 0, -1])    \n",
    "                       \n",
    "            # Check if convergence\n",
    "            if np.allclose(old_policy, self.pi_probabilities, atol=0.0005):\n",
    "                print('Policy iteration converged.')\n",
    "                policy_improvement = True\n",
    "                \n",
    "            # Counter and update epsilon\n",
    "            self.epsilon = 1/(np.sqrt(k + 1.1))\n",
    "            \n",
    "            k += 1\n",
    "            \n",
    "    def update_policy(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        # Ranges\n",
    "        velocity_range = self.velocity_max - self.velocity_min + 1  # Minus for other direction.\n",
    "        velocity_change_range = self.velocity_increase_limit  - self.velocity_decrease_limit  + 1        \n",
    "\n",
    "        # Initialize policy\n",
    "        # NB: limit actions depending on action\n",
    "        self.pi_probabilities = np.zeros((rt.track.shape[0], \n",
    "                                          rt.track.shape[1], \n",
    "                                          velocity_range, \n",
    "                                          velocity_range, \n",
    "                                          velocity_change_range, \n",
    "                                          velocity_change_range), dtype=float)\n",
    "        \n",
    "        # Initialize with equal probabilties for all possible actions        \n",
    "        for y_coord in range(self.pi_probabilities.shape[0]):\n",
    "            for x_coord in range(self.pi_probabilities.shape[1]):\n",
    "                for y_vel in range(self.velocity_min, self.velocity_max + 1):\n",
    "                    for x_vel in range(self.velocity_min, self.velocity_max + 1):\n",
    "                        possible_actions = self.possible_actions((y_vel, x_vel))\n",
    "                        greedy_action = self.greedy_action(state=[y_coord, x_coord, y_vel, x_vel],\n",
    "                                                           possible_actions=possible_actions)\n",
    "#                         if [0, 8, 0, 0] == [y_coord,  x_coord, y_vel, x_vel]:\n",
    "#                             print('--GREEDY ACTION:', greedy_action)\n",
    "                        for y_vel_change, x_vel_change in possible_actions:\n",
    "                            self.pi_probabilities[y_coord, \n",
    "                                                  x_coord, \n",
    "                                                  y_vel, \n",
    "                                                  x_vel, \n",
    "                                                  y_vel_change, \n",
    "                                                  x_vel_change] = self.epsilon_soft_policy(action=[y_vel_change, x_vel_change],\n",
    "                                                                                           greedy_action=greedy_action, \n",
    "                                                                                           all_state_actions=possible_actions)         \n",
    "            \n",
    "    def generate_episode(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        crossed_finishing_line = False\n",
    "        position = self.random_start_position()\n",
    "        first_occurence = defaultdict(int)\n",
    "        \n",
    "        step = 0\n",
    "        while not crossed_finishing_line:\n",
    "#            print('-- Step {}'.format(step))\n",
    "            step += 1\n",
    "#            print('-- Position',  position)\n",
    "            \n",
    "            # Sample action\n",
    "            action = self.sample_action_from_state(position)\n",
    "#            print('-- Action', action)\n",
    "            \n",
    "            # Initiate s, a pair if not already in dict\n",
    "            if str(position + action) not in first_occurence.keys():                \n",
    "                first_occurence[str(position + action)] = step\n",
    "\n",
    "            # Old position\n",
    "            old_position = position.copy()\n",
    "            \n",
    "            # New position\n",
    "            position[0] += position[2] + action[0]\n",
    "            position[1] += position[3] + action[1]\n",
    "            position[2] += action[0]\n",
    "            position[3] += action[1]\n",
    "            \n",
    "#            print('-- New position', position)\n",
    "\n",
    "            # Check if goal if reached (is it in the projected reactangle)\n",
    "            grid_states_to_check = self.get_all_grid_cells_in_projected_retcangle(current_state=[old_position[0], old_position[1]], \n",
    "                                                                                  new_state=[position[0], position[1]])\n",
    "            if self.check_if_goal_is_reached(check_grid_states=grid_states_to_check):\n",
    "                print('-- Goal Reached. Terminating Episode.')\n",
    "                break\n",
    "\n",
    "            # Check if car hits boundery\n",
    "            if position[0] >= self.track.shape[0] or position[1] >= self.track.shape[1]:\n",
    "#                print('Hit the track boundery! (outside matrix)')\n",
    "                position = self.random_start_position()\n",
    "                continue\n",
    "            \n",
    "            new_grid_position = rt.track[position[0], position[1]]\n",
    "            if new_grid_position == 0:\n",
    "#                print('Hit the track boundery !')\n",
    "                position = self.random_start_position()\n",
    "                continue        \n",
    "\n",
    "        print('Steps {}'.format(step))\n",
    "        \n",
    "        return self._get_G_values(first_occurence_dict=first_occurence, total_steps=step)\n",
    "\n",
    "    def check_if_goal_is_reached(self, check_grid_states):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        grid_values = []\n",
    "\n",
    "        for y, x in check_grid_states:\n",
    "            if y <= self.track.shape[0] - 1 and x <= self.track.shape[1] - 1:\n",
    "                grid_values.append(rt.track[y, x])\n",
    "\n",
    "        if 2 in grid_values:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def get_all_grid_cells_in_projected_retcangle(self, current_state, new_state):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        y_coord_current = current_state[0]\n",
    "        x_coord_current = current_state[1]\n",
    "\n",
    "        y_diff = new_state[0] - current_state[0]\n",
    "        x_diff = new_state[1] - current_state[1]\n",
    "\n",
    "        return [[y_coord_current + y, x_coord_current + x] for y in range(0, y_diff + 1) for x in range(0, x_diff + 1)]\n",
    "\n",
    "\n",
    "    def sample_action_from_state(self, state):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        # Action coordinates in probability matrix\n",
    "        array = np.array(['(0, 0)', \n",
    "                          '(0, 1)', \n",
    "                          '(0, 2)', \n",
    "                          '(1, 0)', \n",
    "                          '(1, 1)', \n",
    "                          '(1, 2)',\n",
    "                          '(2, 0)',\n",
    "                          '(2, 1)',\n",
    "                          '(2, 2)'])\n",
    "        \n",
    "        # Randomly pick action\n",
    "#        print('STATE:', state)\n",
    "#        print('---- ! {}'.format(rt.pi_probabilities[state[0], state[1], state[2], state[3]].flatten()))\n",
    "        a = numpy_random.choice(array,\n",
    "                                size=1,\n",
    "                                p=rt.pi_probabilities[state[0], state[1], state[2], state[3]].flatten())        \n",
    "        a = eval(list(a)[0])        \n",
    "        \n",
    "        return self.center_axis_around_zero(coordinates=a, list_range=range(3))\n",
    "\n",
    "    def center_axis_around_zero(self, coordinates, list_range):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        return [x - len(list_range) if x > len(list_range)/2 else x for x in coordinates]\n",
    "\n",
    "    def greedy_action(self, state, possible_actions):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "#         if state == [0, 8, 0, 0]:\n",
    "#             print('possible_actions[0]', possible_actions[0])\n",
    "        greedy_action = possible_actions[0]\n",
    "        q_max = self.q[state[0], state[1], state[2], state[3], greedy_action[0], greedy_action[1]]\n",
    "        \n",
    "        for action in possible_actions:\n",
    "            value = self.q[state[0], state[1], state[2], state[3], action[0], action[1]]\n",
    "#             if state == [0, 8, 0, 0]:            \n",
    "#                 print('action', action)\n",
    "#                 print('value', value)\n",
    "            if value > q_max:\n",
    "                q_max = value\n",
    "                greedy_action = action\n",
    "                    \n",
    "        return self.center_axis_around_zero(coordinates=greedy_action, list_range=range(3))\n",
    "\n",
    "    def epsilon_soft_policy(self, action, greedy_action, all_state_actions):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if action == greedy_action:\n",
    "            return 1 - self.epsilon + self.epsilon/len(all_state_actions)\n",
    "        \n",
    "        return self.epsilon/len(all_state_actions)\n",
    "\n",
    "    def random_start_position(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        grid_position = list(random.choice(np.argwhere(self.track==3)))\n",
    "        velocity = [0, 0]\n",
    "        \n",
    "        return  grid_position + velocity \n",
    "    \n",
    "    def possible_actions(self, velocity):\n",
    "        \"\"\"\n",
    "        Credit: Andreas\n",
    "        \"\"\"\n",
    "        actions = [[a_y, a_x] for a_y in range(-1, 2) for a_x in range(-1, 2)]\n",
    "        legal_actions = []\n",
    "        \n",
    "        v_y, v_x = velocity\n",
    "        \n",
    "        # Discard illegal actions\n",
    "        for a in actions:\n",
    "            a_y, a_x = a\n",
    "            # Cannot go above speed limit in any x direction\n",
    "            if v_x + a_x < self.min_vel or v_x + a_x > self.max_vel:\n",
    "                continue\n",
    "            # Cannot go above speed limit in any y direction\n",
    "            if v_y + a_y < self.min_vel or v_y + a_y > self.max_vel:\n",
    "                continue\n",
    "            # Cannot noop\n",
    "            if v_x + a_x == 0 and v_y + a_y == 0:\n",
    "                continue\n",
    "            legal_actions.append(a)\n",
    "            \n",
    "        return legal_actions\n",
    "\n",
    "    def _get_G_values(self, first_occurence_dict, total_steps):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        G = defaultdict(int)  # Dict. w/ G for first occurence for each s, a pair.\n",
    "\n",
    "        for key, val in first_occurence_dict.items():\n",
    "            number_rewards = total_steps - val\n",
    "\n",
    "            discounted_rewards = []\n",
    "            for k in range(number_rewards):\n",
    "                discounted_rewards.append(self.gamma**k * (-1))\n",
    "\n",
    "            G[key] = sum(discounted_rewards)\n",
    "\n",
    "        return G\n",
    "\n",
    "    @classmethod\n",
    "    def from_csv(cls, file_path):\n",
    "        \n",
    "        file_path = os.path.join(os.getcwd(), file_path)\n",
    "        \n",
    "        track = genfromtxt(file_path, delimiter=',')\n",
    "        track = np.flip(track, axis=0)\n",
    "        \n",
    "        return cls(track) \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 3., 3., 3., 3., 3., 3., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2.],\n",
       "       [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2.],\n",
       "       [0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2.],\n",
       "       [0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2.],\n",
       "       [0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt.track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = get_all_cell_values_in_projected_retcangle([29, 14], [34, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [29, 14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [34, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_cell_values_in_projected_retcangle(current_state, new_state):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    y_coord_current = current_state[0]\n",
    "    x_coord_current = current_state[1]\n",
    "    \n",
    "    y_diff = new_state[0] - current_state[0]\n",
    "    x_diff = new_state[1] - current_state[1]\n",
    "    \n",
    "    return [[y_coord_current + y, x_coord_current + x] for y in range(0, y_diff + 1) for x in range(0, x_diff + 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_goal_is_reached(check_grid_states):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    grid_values = []\n",
    "    \n",
    "    for y, x in check_grid_states:\n",
    "        if y <= rt.track.shape[0] - 1 and x <= rt.track.shape[1] - 1:\n",
    "            grid_values.append(rt.track[y, x])\n",
    "    \n",
    "    if 2 in grid_values:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 16)"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt.track.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If I'm in e.g. 1, 14, -2, 2\n",
    "# And takes action -1, 0\n",
    "\n",
    "# New position -2, 16, -3, 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 3., 3., 3., 3., 3., 3., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2.],\n",
       "       [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2.],\n",
       "       [0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2.],\n",
       "       [0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2.],\n",
       "       [0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2.]])"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt.track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0, 1] = random.choice(rt.all_possible_actions(5, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a Monte Carlo control method to this task to compute \n",
    "# the optimal policy from each starting state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track bounderies:\n",
    "\n",
    "race_track = {'[0, 2]': '[0, 6]',\n",
    "              '[3, 9]': '[-1, 6]',\n",
    "              '[10, 17]': '[-2, 6]',\n",
    "              '[18, 24]':  '[-3, 6]',\n",
    "              '[25, 25]': '[-3, 7]',\n",
    "              '[26, 27]': '[-3, 14]',\n",
    "              '[28, 28]': '[-2,  14]',\n",
    "              '[29, 30]': '[-1, 14]',\n",
    "              '[31, 31]': '[0, 14]'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# States:\n",
    "states = []\n",
    "for key in race_track.keys():\n",
    "    for x_coord in range(eval(key)[0], eval(key)[1] + 1):\n",
    "        for y_coord in range(eval(race_track[key])[0], eval(race_track[key])[1] + 1):\n",
    "            states.append([x_coord, y_coord])\n",
    "            \n",
    "# Start states:\n",
    "start_states = [[0, 0],\n",
    "                [0, 1],\n",
    "                [0, 2],\n",
    "                [0, 3],\n",
    "                [0, 4],\n",
    "                [0, 5],\n",
    "                [0, 6]]\n",
    "\n",
    "# End states:\n",
    "end_states = [[x, 14] for x in range(26, 32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RaceTrackingMDP:\n",
    "    def __init__(self,\n",
    "                 states,\n",
    "                 start_states,\n",
    "                 end_states,\n",
    "                 zero_velocity_prob=0.9,\n",
    "                 velocity_max=4,\n",
    "                 speed_decrease_limit=-1,\n",
    "                 speed_increase_limit=1):\n",
    "        self.states = states\n",
    "        self.start_states = start_states\n",
    "        self.end_states = end_states\n",
    "        self.zero_velocity_prob = zero_velocity_prob\n",
    "        self.velocity_max = velocity_max\n",
    "        self.speed_decrease_limit = speed_decrease_limit\n",
    "        self.speed_increase_limit = speed_increase_limit\n",
    "        \n",
    "        self.q_values = [0 for x in self.states for a in range(0, 9 + 1)]\n",
    "        self.returns = [[] for x in self.states for a in range(0, 9 + 1)]\n",
    "        self.policies = [[1/9] * 6 for x in self.states for a in range(0, 9 + 1)]\n",
    "        \n",
    "    def actions(self, x_coord, y_coord):\n",
    "        \"\"\"Iterator over all actions\"\"\"\n",
    "        \n",
    "        for a1 in range(self.speed_decrease_limit, self.speed_increase_limit + 1):\n",
    "            for a2 in range(self.speed_decrease_limit, self.speed_increase_limit + 1):\n",
    "                yield a1 + x_coord, a2 + y_coord\n",
    "                \n",
    "    def \n",
    "                \n",
    "    def policy_iteration(self):\n",
    "        \"\"\"Iterates over policies\"\"\"\n",
    "        \n",
    "        for iteration in range(0, 10):\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = RaceTrackingMDP(states=states, start_states=start_states, end_states=end_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((2, 4, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
